{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83WXBfLP19P1",
        "colab_type": "text"
      },
      "source": [
        "Downloads and installs the dependencies required for TA-Lib and TA-Lib, Wikipedia where we get our list of the Dow Jones Industrial Average and the Yahoo Finance API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ6zfl3yXcuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "print('Beginning file download with urllib2...')\n",
        "url = 'https://netcologne.dl.sourceforge.net/project/ta-lib/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz'\n",
        "urllib.request.urlretrieve(url, '/content/ta-lib-0.4.0-src.tar.gz')\n",
        "print(\"Download complete. Unpacking...\")\n",
        "!tar -xzf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ./ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!sudo make install\n",
        "%cd /content\n",
        "!pip install TA-Lib\n",
        "!pip install yfinance\n",
        "!pip install wikipedia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmXsz59E14P-",
        "colab_type": "text"
      },
      "source": [
        "Imports the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtbeXZEEYiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import sklearn.metrics as metrics\n",
        "import os\n",
        "from os import path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "keras = tf.keras\n",
        "import yfinance as yf\n",
        "import wikipedia as wp\n",
        "import talib\n",
        "from talib.abstract import *\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj5CwiW02sR3",
        "colab_type": "text"
      },
      "source": [
        "Scrapes the wikipedia page of the Dow Jones Index and gets the companies which are currently in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EDhNV_st9lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dow_jones_tickers = []\n",
        "html = wp.page(\"Dow_Jones_Industrial_Average\").html().encode(\"UTF-8\")\n",
        "try: \n",
        "    df = pd.read_html(html)[1]  # Try 2nd table first as most pages contain contents table first\n",
        "except IndexError:\n",
        "    df = pd.read_html(html)[0]\n",
        "\n",
        "for symbol in df['Symbol']:\n",
        "  is_there = symbol.find('NYSE:')\n",
        "  #print(symbol)\n",
        "  if is_there == 0:\n",
        "    symbol = symbol.lstrip('NYSE:').strip()\n",
        "  \n",
        "  dow_jones_tickers.append(symbol)\n",
        "  \n",
        "print(len(dow_jones_tickers))\n",
        "print(dow_jones_tickers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0jmVXtJLYbn",
        "colab_type": "text"
      },
      "source": [
        "This cell downloads the price data for the companies in the Dow Jones Index and saves it as csv to a path. This only needs to be run once a day after market close to update the prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3G6XnciGQJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for ticker in dow_jones_tickers:\n",
        "#   curr_ticker = yf.Ticker(ticker)\n",
        "#   ticker_history = curr_ticker.history(period='max', interval = '1d')\n",
        "#   ticker_history.drop(columns=['Dividends', 'Stock Splits'], inplace=True)\n",
        "#   ticker_history.to_csv('/content/drive/My Drive/Data Sets/Price Data/%s_price_data.csv' %ticker)\n",
        "#   time.sleep(1)\n",
        "\n",
        "# df_display = pd.read_csv('/content/drive/My Drive/Data Sets/Price Data/AAPL_price_data.csv')\n",
        "# df_display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wvZLzaijS-C",
        "colab_type": "text"
      },
      "source": [
        "Generates a dataset of technical indicators for a company based on how many days in the past we want to look when training the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXBte7lxp7Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_technical_data_single(ticker, days):\n",
        "  \n",
        "  path_name = os.getcwd() + '/Technical Sets/%s' %ticker\n",
        "  print(path_name)\n",
        "  if path.exists(path_name) == False:\n",
        "    os.makedirs(path_name)\n",
        "  else:\n",
        "    shutil.rmtree(path_name)\n",
        "    os.mkdir(path_name)\n",
        "  \n",
        "  generation_start = time.time()\n",
        "\n",
        "  #Change to own path. Standard Open, High, Low, Close data generated by yfinance.\n",
        "  #Dividends and stock splits columns dropped.\n",
        "  ticker_history = pd.read_csv('/content/drive/My Drive/Data Sets/Price Data/%s_price_data.csv' %ticker)\n",
        "  ticker_history.set_index('Date', inplace=True)\n",
        "    \n",
        "  inputs = {\n",
        "      'open' : ticker_history['Open'].to_numpy(),\n",
        "      'high' : ticker_history['High'].to_numpy(),\n",
        "      'low'  : ticker_history['Open'].to_numpy(),\n",
        "      'close': ticker_history['Close'].to_numpy(),\n",
        "      'volume': ticker_history['Volume'].astype(float)\n",
        "  }\n",
        "    \n",
        "  tasize = talib.get_function_groups()\n",
        "  count = 0 \n",
        "  total_count = 0\n",
        "  for group in tasize:    \n",
        "      \n",
        "    for indicator in tasize[group]:\n",
        "          # There is a bug which does no allow the creation of MAVP indicator.\n",
        "          if indicator == 'MAVP':\n",
        "            continue\n",
        "            \n",
        "          method = getattr(talib.abstract, indicator)\n",
        "          output = method(inputs, timeperiod = days)\n",
        "            \n",
        "          if len(output) == len(ticker_history):\n",
        "            ticker_history[indicator] = output\n",
        "            count+=1\n",
        "            total_count += count * len(ticker_history)\n",
        "  print(\"Indicators created for\", ticker, \": \", count)\n",
        "  ticker_history.to_csv(path_name +'/%s_technical_indicators.csv' %ticker)\n",
        "\n",
        "  #print(\"Installation of libraries and generation of data sets took: \", (time.time()-start_time))\n",
        "  print(\"Total number of indicators created: \", total_count)\n",
        "  print(\"Generation of datasets in seconds took: \", (time.time()-generation_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExtGElCa3vhB",
        "colab_type": "text"
      },
      "source": [
        "Loads a dataset with technical indicators and selects the window of time we want to look at."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCdA3VO1ub3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_dataset(ticker):\n",
        "  path_to = '/content/Technical Sets/'+ ticker + '/%s_technical_indicators.csv' %ticker\n",
        "  #path_to = '/content/drive/My Drive/Data Sets/Price Data/%s_price_data.csv' %ticker \n",
        "  print(path_to)\n",
        "  company_indicators = pd.read_csv(path_to)\n",
        "  start_date = datetime.strptime('01-01-2000', '%d-%m-%Y')\n",
        "  end_date = datetime.strptime('01-01-2011', '%d-%m-%Y')\n",
        "  company_indicators['Date'] = pd.to_datetime(company_indicators['Date'])\n",
        "  mask = (company_indicators['Date'] >= start_date) & (company_indicators['Date'] <= end_date)\n",
        "  company_indicators = company_indicators.loc[mask]\n",
        "  company_indicators.reset_index(inplace=True)\n",
        "  company_indicators.drop(columns='index', inplace=True)\n",
        "  company_indicators.drop(columns='Date', inplace=True)  \n",
        "  return company_indicators\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on8ThIBv4KNu",
        "colab_type": "text"
      },
      "source": [
        "After we have selected our time frame we generate a number of sliding window datasets used for training and predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0kZ1Z3ddBjx",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def generate_sets(ds, window_size):\n",
        "  row = 0\n",
        "  \n",
        "  labels = []\n",
        "  array_of_arrays = []\n",
        "  \n",
        "  ds = ds.fillna(0)\n",
        "  \n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  for column in ds.columns:\n",
        "    #print(column)\n",
        "    series = np.array(ds[column])\n",
        "    series = series.reshape((-1,1))\n",
        "    series = scaler.fit_transform(series)\n",
        "    series = series.flatten()\n",
        "    ds[column] = series\n",
        "  print(\"Preprocessing done\")\n",
        "  \n",
        "  while row < len(ds):\n",
        "    \n",
        "    if row >= window_size:\n",
        "      window_start = row - window_size\n",
        "      window_end = row\n",
        "      window_array = []\n",
        "      \n",
        "      for item in range(window_start, window_end):\n",
        "        window_array.append(ds.iloc[item].to_numpy())\n",
        "      \n",
        "      labels.append(ds.at[row,'Close'])\n",
        "      array_of_arrays.append(window_array)\n",
        "    \n",
        "    row+=1\n",
        "\n",
        "  print(\"Data shape:\", np.array(array_of_arrays).shape, \n",
        "        \"Labels shape:\", np.array(labels).shape)\n",
        "  \n",
        "  return np.array(array_of_arrays), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by4eqzvU4uMJ",
        "colab_type": "text"
      },
      "source": [
        "We split the generated sliding windows datasets and labels into training, validation and testing sets. We plot and save the plots of the closing prices of all 3 sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecKsMN_Ko3TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_train_val_test_single(days, ticker, save_path):\n",
        "  gen_technical_data_single(ticker, days)\n",
        "  transformed_dataset = transform_dataset(ticker)\n",
        "  data, labels = generate_sets(transformed_dataset, days)\n",
        "  \n",
        "  # features_train = data[:-5]\n",
        "  # labels_train = labels[:-5]\n",
        "  # features_test = data[(len(data)-5):]\n",
        "  # labels_test = labels[(len(labels)-5):]\n",
        "\n",
        "  # print(len(features_train))\n",
        "  # print(len(features_test))\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(data, labels, test_size = 0.10, shuffle = False, random_state = 42)\n",
        "  features_train, validation_data, labels_train, labels_validation = train_test_split(features_train, labels_train, test_size = 0.15, shuffle = False, random_state = 42 )\n",
        "\n",
        "  print(\"Train\")\n",
        "  print(len(features_train))\n",
        "  print(len(labels_train))\n",
        "  plt.plot(labels_train)\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Scaled Price')\n",
        "  plt.savefig((save_path + '/train.png'))\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  print(\"Validation\")\n",
        "  print(len(validation_data))\n",
        "  print(len(labels_validation))\n",
        "  plt.plot(labels_validation)\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Scaled Price')\n",
        "  plt.savefig((save_path + '/validation.png'))\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "  print(\"Test\")\n",
        "  print(len(features_test))\n",
        "  print(len(labels_test))\n",
        "  plt.plot(labels_test)\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Scaled Price')\n",
        "  plt.savefig((save_path + '/test.png'))\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  return features_train, validation_data, features_test, labels_train, labels_validation, labels_test, transformed_dataset "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-KVql0p5Jar",
        "colab_type": "text"
      },
      "source": [
        "After we have completed training and predictions we reverse the scaling on the predictions and testin dataset in order to see the actual values. We calculate and save the means of both predictions and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESxOFWA_yi6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_scaling(dataset, predictions, l_test, path):\n",
        "  scaler_2 = preprocessing.MinMaxScaler()\n",
        "  \n",
        "  scale_it = scaler_2.fit(np.array(dataset['Close']).reshape((-1,1)))\n",
        "  \n",
        "  test_labels_inversed = scale_it.inverse_transform(l_test.reshape((-1,1)))\n",
        "  test_labels_inversed = test_labels_inversed.flatten()\n",
        "  \n",
        "  predictions_inversed = scale_it.inverse_transform(predictions)\n",
        "  predictions_inversed = predictions_inversed.flatten()\n",
        "  \n",
        "  pred_str = \"Predictions mean: %f\"  %predictions_inversed.mean()\n",
        "  test_str = \"Test labels mean: %f\"  %test_labels_inversed.mean()\n",
        "\n",
        "\n",
        "  filepath = '%s/means.txt' %path\n",
        "  print(filepath) \n",
        "  f = open(filepath, 'w+')\n",
        "  f.write(str(pred_str + '\\n'))\n",
        "  f.write(str(test_str))\n",
        "  f.close()\n",
        "  print(str(pred_str))\n",
        "  print(str(test_str))\n",
        "  \n",
        "  return predictions_inversed, test_labels_inversed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzzbIm6u59R-",
        "colab_type": "text"
      },
      "source": [
        "We generate the paths and make the folders where the logs of training will be stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4-Lj2rdRJFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def path_to_log():\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "  str_path = '/content/drive/My Drive/Logs LSTM/%s' %current_time\n",
        "  os.makedirs(str_path)\n",
        "  return str_path\n",
        "\n",
        "def spec_path(ticker, days, gen_path):\n",
        "  if days < 10:\n",
        "    days = '0' + str(days)\n",
        "  spec_path = '/%s/%s' %(ticker, days)\n",
        "  #print(gen_path + spec_path)\n",
        "  os.makedirs(gen_path + spec_path)\n",
        "  return gen_path + spec_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBBOC6Mn6zys",
        "colab_type": "text"
      },
      "source": [
        "After training and predictions are completed we plot the predictions and testing labels to see how well the algorithm did. We also save the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABD-WVRa0hXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_results(predictions_descaled, l_test_descaled, save_path):\n",
        "  plt.figure(figsize=(15, 10))\n",
        "  plt.plot(l_test_descaled, label = 'Price')\n",
        "  plt.plot(predictions_descaled, label = 'Prediction')\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Price')\n",
        "  plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
        "  plt.savefig((save_path + '/comparion_plot.png'))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7W8TDd7sIA",
        "colab_type": "text"
      },
      "source": [
        "After the predictions are complete we calculate how many of the days as a percentage the algorithm managed to correctly predict the direction of the price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVAV_bY6eJ3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_correct_directions(predictions_descaled, l_test_descaled, path):\n",
        "  day_label = 0\n",
        "  pred_label = 0\n",
        "  count_correct = 0\n",
        "  for day in range(len(predictions_descaled)):\n",
        "    if day >= 1:\n",
        "      if l_test_descaled[day] - l_test_descaled[day-1] > 0:\n",
        "        day_label = 1\n",
        "      else:\n",
        "        day_label = 0\n",
        "      \n",
        "      if predictions_descaled[day] - predictions_descaled[day - 1] > 0:\n",
        "        pred_label = 1\n",
        "      else :\n",
        "        pred_label = 0\n",
        "\n",
        "      if pred_label == day_label: \n",
        "        count_correct += 1\n",
        "\n",
        "\n",
        "  accuracy_movement = count_correct/len(predictions_descaled)\n",
        "  f = open(str(path + '/precent_correct_directions.txt'), 'w+')\n",
        "  f.write(str(\"Percentage of accurately predicted next day directions\" + str(accuracy_movement)))\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wyIq1KX78ar",
        "colab_type": "text"
      },
      "source": [
        "We set the parameters and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi2o-brCrGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(f_train):\n",
        "  keras.backend.clear_session()\n",
        "  tf.random.set_seed(42)\n",
        "  model = keras.models.Sequential([\n",
        "                                  keras.layers.LSTM(units=149, return_sequences=True,\n",
        "                                                    input_shape = (f_train.shape[1],\n",
        "                                                                    f_train.shape[2])),\n",
        "                                  keras.layers.Dropout(rate = 0.5), \n",
        "                                  keras.layers.LSTM(units=75, return_sequences=True),\n",
        "                                  keras.layers.Dropout(rate = 0.5),\n",
        "                                  keras.layers.LSTM(units=35, return_sequences=False),\n",
        "                                  keras.layers.Dense(units = 1)\n",
        "  ])\n",
        "\n",
        "\n",
        "  model.compile(loss='mse', optimizer = 'adam', metrics=['mae', 'mse'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltIISCas8ApR",
        "colab_type": "text"
      },
      "source": [
        "We loop through all companies in the Dow Jones Index, create datasets for between 3 and 15 days looking in the past then make train a model on for the company and days and save the data generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lGpvHJqCKTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_save = path_to_log()\n",
        "# for ticker in dow_jones_tickers:\n",
        "  \n",
        "#    if ticker == 'DOW' or ticker == 'V':\n",
        "#       continue\n",
        "\n",
        "ticker = 'HD'\n",
        "for days in range(3, 15+1):\n",
        "\n",
        "    spec_path_to_save = spec_path(ticker, days, path_to_save)\n",
        "    print(\"LOG PATH: \" + spec_path_to_save)\n",
        "    f_train, f_val, f_test, l_train, l_val, l_test, dataset = gen_train_val_test_single(days, ticker, spec_path_to_save)\n",
        "    \n",
        "    model = compile_model(f_train)\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint('%s/best_model.h5' %spec_path_to_save, save_best_only=True)\n",
        "    tensorboard_path = spec_path_to_save + '/tensorboard/'\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = tensorboard_path, histogram_freq=1) \n",
        "    \n",
        "    model.fit(f_train, l_train, epochs=50, batch_size = 62,\n",
        "                        shuffle= False,\n",
        "                        callbacks = [tensorboard_callback],\n",
        "                        verbose = 0)\n",
        "    #history = keras.models.load_model('%s/best_model.h5' %spec_path_to_save)\n",
        "    predictions = model.predict(f_test)  \n",
        "    \n",
        "    predictions_descaled, l_test_descaled = reverse_scaling(dataset, predictions, l_test, spec_path_to_save)\n",
        "    plot_results(predictions_descaled, l_test_descaled, spec_path_to_save)\n",
        "    count_correct_directions(predictions_descaled, l_test_descaled, spec_path_to_save)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}